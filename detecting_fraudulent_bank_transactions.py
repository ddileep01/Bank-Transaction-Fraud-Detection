# -*- coding: utf-8 -*-
"""IDS_Assignment_1_balnk.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hINf1XPElf-kVup5CBFAdE5r79ZBkbm8

# <center>        **Introduction to Data Science (S2-22_DSECLZG532)-ASSIGNMENT**</center>

## Group No

## Group Member Names:
1.
2.
3.
4.

# 1. Business Understanding

Students are expected to identify an analytical problem of your choice. You have to detail the Business Understanding part of your problem under this heading which basically addresses the following questions.

   1. What is the business problem that you are trying to solve?
   2. What data do you need to answer the above problem?
   3. What are the different sources of data?
   4. What kind of analytics task are you performing?

Score: 1 Mark in total (0.25 mark each)

--------------Type the answers below this line--------------

# 2. Data Acquisition

For the problem identified , find an appropriate data set (Your data set must
be unique with minimum **20 features and 10k rows**) from any public data source.

---



## 2.1 Download the data directly
"""

##---------Type the code below this line------------------##

# Kaggle API setup and dataset download
import os
import zipfile

os.environ["KAGGLE_USERNAME"] = "swarali01234"
os.environ["KAGGLE_KEY"] = "3f020ae542014e3aaa65015f95fc62c8"

!kaggle datasets download -d marusagar/bank-transaction-fraud-detection -p ./data --unzip

# Verify files inside the extracted folder
import os
print("Files in dataset folder:", os.listdir("./data"))

"""## 2.2 Code for converting the above downloaded data into a dataframe"""

##---------Type the code below this line------------------##

import pandas as pd
import os

# List files in the dataset folder
files = os.listdir("./data")
print("Available files:", files)

# Identify the correct CSV file dynamically
csv_file = [file for file in files if file.endswith(".csv")][0]  # Pick the first CSV file
csv_path = os.path.join("./data", csv_file)

# Load the dataset
df = pd.read_csv(csv_path)

# Confirm data loading
print("Data Loaded Successfully!")
print("Dataset Shape:", df.shape)

"""## 2.3 Confirm the data has been correctly by displaying the first 5 and last 5 records."""

##---------Type the code below this line------------------##

# Display the first and last 5 records
print(df.head())
print(df.tail())

"""## 2.4 Display the column headings, statistical information, description and statistical summary of the data."""

##---------Type the code below this line------------------##

# Display column names, statistical summary, and dataset info
print("Columns:", df.columns)
print("\nStatistical Summary:\n", df.describe())
print("\nDataset Info:\n")
df.info()

"""## 2.5 Write your observations from the above.
1. Size of the dataset
2. What type of data attributes are there?
3. Is there any null data that has to be cleaned?

Score: 2 Marks in total (0.25 marks for 2.1, 0.25 marks for 2.2, 0.5 marks for 2.3, 0.25 marks for 2.4, 0.75 marks for 2.5)

--------------Type the answers below this line--------------

# 3. Data Preparation

If input data is numerical or categorical, do 3.1, 3.2 and 3.4
If input data is text, do 3.3 and 3.4

## 3.1 Check for

* duplicate data
* missing data
* data inconsistencies
"""

##---------Type the code below this line------------------##

# Check for duplicate records
duplicate_count = df.duplicated().sum()
print(f"Number of duplicate records: {duplicate_count}")

# Check for missing values
missing_values = df.isnull().sum()
print("\nMissing values in each column:")
print(missing_values[missing_values > 0])

# Check for data inconsistencies (e.g., negative values in columns that should be positive)
numeric_columns = df.select_dtypes(include=['number']).columns
inconsistent_values = df[(df[numeric_columns] < 0).any(axis=1)]
print("\nNumber of rows with inconsistent values (negative where not expected):", len(inconsistent_values))

"""## 3.2 Apply techiniques
* to remove duplicate data
* to impute or remove missing data
* to remove data inconsistencies

"""

##---------Type the code below this line------------------##

# Remove duplicate records
df_cleaned = df.drop_duplicates()
print(f"Duplicate records removed: {df.shape[0] - df_cleaned.shape[0]}")

# Handling missing values: Drop columns with too many missing values (>50%) or fill missing data
threshold = 0.5 * df_cleaned.shape[0]
df_cleaned = df_cleaned.dropna(thresh=threshold, axis=1)  # Drop columns with too many missing values

# Fill remaining missing values with column median for numerical and mode for categorical data
for col in df_cleaned.columns:
    if df_cleaned[col].isnull().sum() > 0:
        if df_cleaned[col].dtype == "object":  # Categorical columns
            df_cleaned[col].fillna(df_cleaned[col].mode()[0], inplace=True)
        else:  # Numerical columns
            df_cleaned[col].fillna(df_cleaned[col].median(), inplace=True)

print("Missing values handled.")

# Removing inconsistent data (e.g., negative amounts in transaction amounts)
df_cleaned = df_cleaned[(df_cleaned[numeric_columns] >= 0).all(axis=1)]
print("Inconsistent data removed.")

"""## 3.3 Encode categorical data"""

##---------Type the code below this line------------------##

from sklearn.preprocessing import LabelEncoder

# Identify categorical columns
categorical_cols = df_cleaned.select_dtypes(include=['object']).columns
print("Categorical columns to encode:", categorical_cols.tolist())

# Apply Label Encoding
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df_cleaned[col] = le.fit_transform(df_cleaned[col])
    label_encoders[col] = le  # Store encoders for inverse transformation if needed

print("Categorical data encoding completed.")

##---------Type the code below this line------------------##

"""## 3.4 Report

Mention and justify the method adopted
* to remove duplicate data, if present
* to impute or remove missing data, if present
* to remove data inconsistencies, if present

OR for textdata
* How many tokens after step 3?
* how may tokens after stop words filtering?

If the any of the above are not present, then also add in the report below.

Score: 2 Marks (based on the dataset you have, the data prepreation you had to do and report typed, marks will be distributed between 3.1, 3.2, 3.3 and 3.4)
"""

##---------Type the code below this line------------------##

report = ""

# Duplicate Data
report += f"Duplicate records removed: {df.shape[0] - df_cleaned.shape[0]}\n"

# Missing Data
report += "Missing data handling: Columns with too many missing values were dropped, others were imputed with median/mode.\n"

# Data Inconsistencies
report += "Data inconsistencies handled: Negative values in numerical columns were removed.\n"

# Categorical Encoding
report += f"Categorical columns encoded using Label Encoding: {', '.join(categorical_cols)}\n"

# If dataset involved text processing:
text_column = "your_text_column"  # Replace with actual text column if applicable
if text_column in df_cleaned.columns:
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    import nltk

    nltk.download("punkt")
    nltk.download("stopwords")

    df_cleaned["tokens"] = df_cleaned[text_column].apply(lambda x: word_tokenize(str(x).lower()))
    report += f"\nTokens after step 3: {df_cleaned['tokens'].apply(len).sum()}\n"

    stop_words = set(stopwords.words("english"))
    df_cleaned["filtered_tokens"] = df_cleaned["tokens"].apply(lambda x: [word for word in x if word not in stop_words])
    report += f"Tokens after stop words filtering: {df_cleaned['filtered_tokens'].apply(len).sum()}\n"

print(report)

##---------Type the code below this line------------------##

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Identify the target column
target_column = "Is_Fraud"

# Check unique values in target column
print(f"Unique values in '{target_column}': {df_cleaned[target_column].unique()}")

# Separate features (X) and target (y)
X = df_cleaned.drop(columns=[target_column])  # Features
y = df_cleaned[target_column]  # Target

# Encode target variable if it is categorical
if y.dtype == 'object':
    le = LabelEncoder()
    y = le.fit_transform(y)
    print(f"Target variable '{target_column}' encoded using Label Encoding.")

# Display dataset shape
print(f"Features shape: {X.shape}, Target shape: {y.shape}")

# Report Observations
report = f"Target Variable Identified: {target_column}\n"
if y.dtype != "object":
    report += "Target variable was numeric, no encoding was required.\n"
else:
    report += "Target variable was categorical and was encoded using Label Encoding.\n"
report += f"Features (X) shape: {X.shape}\nTarget (y) shape: {y.shape}\n"

print("\n===== Report =====\n", report)

"""## 3.5 Identify the target variables.

* Separate the data from the target such that the dataset is in the form of (X,y) or (Features, Label)

* Discretize / Encode the target variable or perform one-hot encoding on the target or any other as and if required.

* Report the observations

Score: 1 Mark

# 4. Data Exploration using various plots

## 4.1 Scatter plot of each quantitative attribute with the target.

Score: 1 Mark
"""

##---------Type the code below this line------------------##

"""## 4.2 EDA using visuals
* Use (minimum) 2 plots (pair plot, heat map, correlation plot, regression plot...) to identify the optimal set of attributes that can be used for classification.
* Name them, explain why you think they can be helpful in the task and perform the plot as well. Unless proper justification for the choice of plots given, no credit will be awarded.

Score: 2 Marks
"""

##---------Type the code below this line------------------##

"""# 5. Data Wrangling

## 5.1 Univariate Filters

#### Numerical and Categorical Data
* Identify top 5 significant features by evaluating each feature independently with respect to the target/other variable by exploring
1. Mutual Information (Information Gain)
2. Gini index
3. Gain Ratio
4. Chi-Squared test
5. Strenth of Association

(From the above 5 you are required to use only any <b>two</b>)



Score: 3 Marks
"""

##---------Type the code below this line------------------##

"""## 5.2 Report observations

Write your observations from the results of each method. Clearly justify your choice of the method.

Score 1 mark
"""

##---------Type the code below this line------------------##

"""# 6. Implement Machine Learning Techniques

Use any 2 ML tasks
1. Classification  

2. Clustering  

3. Association Analysis

4. Anomaly detection

You may use algorithms included in the course, e.g. Decision Tree, K-means etc. or an algorithm you learnt on your own with a brief explanation.
A clear justification have to be given for why a certain algorithm was chosen to address your problem.

Score: 4 Marks (2 marks each for each algorithm)

## 6.1 ML technique 1 + Justification
"""

##---------Type the code below this line------------------##

"""## 6.2 ML technique 2 + Justification"""

##---------Type the code below this line------------------##

"""## 7. Conclusion

Compare the performance of the ML techniques used.

Derive values for preformance study metrics like accuracy, precision, recall, F1 Score, AUC-ROC etc to compare the ML algos and plot them. A proper comparision based on different metrics should be done and not just accuracy alone, only then the comparision becomes authentic. You may use Confusion matrix, classification report, Word cloud etc as per the requirement of your application/problem.

Score 1 Mark
"""

##---------Type the code below this line------------------##

"""## 8. Solution

What is the solution that is proposed to solve the business problem discussed in Section 1. Also share your learnings while working through solving the problem in terms of challenges, observations, decisions made etc.

Score 2 Marks

--------------Type the answers below this line--------------

##---------Type the answer below this line------------------##

##NOTE
All Late Submissions will incur a penalty of -2 marks. Do ensure on time submission to avoid penalty.

Good Luck!!!
"""
